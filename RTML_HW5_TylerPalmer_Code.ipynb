{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IlBEUqMN07I"
      },
      "outputs": [],
      "source": [
        "# Tyler Palmer\n",
        "# Student ID: 801058786\n",
        "# Github: https://github.com/TPal49\n",
        "# Homework 5\n",
        "\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import math\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install setuptools==66\n",
        "!pip install d2l==1.0.0-beta0"
      ],
      "metadata": {
        "id": "f1JEKTI6N_X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from d2l import torch as d2l\n",
        "d2l.use_svg_display()"
      ],
      "metadata": {
        "id": "L2zxMq3sOAq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_seq2seq(module):  \n",
        "    \"\"\"Initialize weights for Seq2Seq.\"\"\"\n",
        "    if type(module) == nn.Linear:\n",
        "         nn.init.xavier_uniform_(module.weight)\n",
        "    if type(module) == nn.GRU:\n",
        "        for param in module._flat_weights_names:\n",
        "            if \"weight\" in param:\n",
        "                nn.init.xavier_uniform_(module._parameters[param])\n",
        "\n",
        "class Seq2SeqEncoder(d2l.Encoder):  \n",
        "    \"\"\"The RNN encoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\n",
        "        self.apply(init_seq2seq)\n",
        "\n",
        "    def forward(self, X, *args):\n",
        "        # X shape: (batch_size, num_steps)\n",
        "        embs = self.embedding(X.t().type(torch.int64))\n",
        "        # embs shape: (num_steps, batch_size, embed_size)\n",
        "        outputs, state = self.rnn(embs)\n",
        "        # outputs shape: (num_steps, batch_size, num_hiddens)\n",
        "        # state shape: (num_layers, batch_size, num_hiddens)\n",
        "        return outputs, state\n",
        "\n",
        "vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2\n",
        "batch_size, num_steps = 4, 9\n",
        "encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
        "X = torch.zeros((batch_size, num_steps))\n",
        "enc_outputs, enc_state = encoder(X)\n",
        "d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))\n",
        "\n",
        "d2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))\n",
        "\n",
        "class Seq2SeqDecoder(d2l.Decoder):\n",
        "    \"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens,\n",
        "                           num_layers, dropout)\n",
        "        self.dense = nn.LazyLinear(vocab_size)\n",
        "        self.apply(init_seq2seq)\n",
        "\n",
        "    def init_state(self, enc_all_outputs, *args):\n",
        "        return enc_all_outputs\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # X shape: (batch_size, num_steps)\n",
        "        # embs shape: (num_steps, batch_size, embed_size)\n",
        "        embs = self.embedding(X.t().type(torch.int32))\n",
        "        enc_output, hidden_state = state\n",
        "        # context shape: (batch_size, num_hiddens)\n",
        "        context = enc_output[-1]\n",
        "        # Broadcast context to (num_steps, batch_size, num_hiddens)\n",
        "        context = context.repeat(embs.shape[0], 1, 1)\n",
        "        # Concat at the feature dimension\n",
        "        embs_and_context = torch.cat((embs, context), -1)\n",
        "        outputs, hidden_state = self.rnn(embs_and_context, hidden_state)\n",
        "        outputs = self.dense(outputs).swapaxes(0, 1)\n",
        "        # outputs shape: (batch_size, num_steps, vocab_size)\n",
        "        # hidden_state shape: (num_layers, batch_size, num_hiddens)\n",
        "        return outputs, [enc_output, hidden_state]\n",
        "\n",
        "decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n",
        "state = decoder.init_state(encoder(X))\n",
        "dec_outputs, state = decoder(X, state)\n",
        "d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\n",
        "d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))\n",
        "\n",
        "class Seq2Seq(d2l.EncoderDecoder):  \n",
        "    \"\"\"The RNN encoder-decoder for sequence to sequence learning.\"\"\"\n",
        "    def __init__(self, encoder, decoder, tgt_pad, lr):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        Y_hat = self(*batch[:-1])\n",
        "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Adam optimizer is used here\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "@d2l.add_to_class(Seq2Seq)\n",
        "def loss(self, Y_hat, Y):\n",
        "    l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n",
        "    mask = (Y.reshape(-1) != self.tgt_pad).type(torch.float32)\n",
        "    return (l * mask).sum() / mask.sum()\n",
        "\n",
        "@d2l.add_to_class(d2l.EncoderDecoder)  \n",
        "def predict_step(self, batch, device, num_steps,\n",
        "                 save_attention_weights=False):\n",
        "    batch = [a.to(device) for a in batch]\n",
        "    src, tgt, src_valid_len, _ = batch\n",
        "    enc_all_outputs = self.encoder(src, src_valid_len)\n",
        "    dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)\n",
        "    outputs, attention_weights = [tgt[:, 0].unsqueeze(1), ], []\n",
        "    for _ in range(num_steps):\n",
        "        Y, dec_state = self.decoder(outputs[-1], dec_state)\n",
        "        outputs.append(Y.argmax(2))\n",
        "        # Save attention weights (to be covered later)\n",
        "        if save_attention_weights:\n",
        "            attention_weights.append(self.decoder.attention_weights)\n",
        "    return torch.cat(outputs[1:], 1), attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "YJ5lYLfJQRGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bleu(pred_seq, label_seq, k):  \n",
        "    \"\"\"Compute the BLEU.\"\"\"\n",
        "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
        "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
        "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
        "    for n in range(1, min(k, len_pred) + 1):\n",
        "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
        "        for i in range(len_label - n + 1):\n",
        "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
        "        for i in range(len_pred - n + 1):\n",
        "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
        "                num_matches += 1\n",
        "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
        "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
        "    return score"
      ],
      "metadata": {
        "id": "yPxDIIb6RJEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.MTFraEng(batch_size=128)\n",
        "embed_size, num_hiddens, num_layers, dropout = 64, 64, 3, 0.2\n",
        "encoder = Seq2SeqEncoder(len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqDecoder(len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],lr=0.005)\n",
        "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "jzMzuSULQ-iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder(d2l.Decoder): \n",
        "    \"\"\"The base attention-based decoder interface.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
        "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
        "                 dropout=0):\n",
        "        super().__init__()\n",
        "        self.attention = d2l.AdditiveAttention(num_hiddens, dropout)\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.GRU(\n",
        "            embed_size + num_hiddens, num_hiddens, num_layers,\n",
        "            dropout=dropout)\n",
        "        self.dense = nn.LazyLinear(vocab_size)\n",
        "        self.apply(d2l.init_seq2seq)\n",
        "\n",
        "    def init_state(self, enc_outputs, enc_valid_lens):\n",
        "        # Shape of outputs: (num_steps, batch_size, num_hiddens).\n",
        "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
        "        outputs, hidden_state = enc_outputs\n",
        "        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)\n",
        "\n",
        "    def forward(self, X, state):\n",
        "        # Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n",
        "        # Shape of hidden_state: (num_layers, batch_size, num_hiddens)\n",
        "        enc_outputs, hidden_state, enc_valid_lens = state\n",
        "        # Shape of the output X: (num_steps, batch_size, embed_size)\n",
        "        X = self.embedding(X).permute(1, 0, 2)\n",
        "        outputs, self._attention_weights = [], []\n",
        "        for x in X:\n",
        "            # Shape of query: (batch_size, 1, num_hiddens)\n",
        "            query = torch.unsqueeze(hidden_state[-1], dim=1)\n",
        "            # Shape of context: (batch_size, 1, num_hiddens)\n",
        "            context = self.attention(\n",
        "                query, enc_outputs, enc_outputs, enc_valid_lens)\n",
        "            # Concatenate on the feature dimension\n",
        "            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)\n",
        "            # Reshape x as (1, batch_size, embed_size + num_hiddens)\n",
        "            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)\n",
        "            outputs.append(out)\n",
        "            self._attention_weights.append(self.attention.attention_weights)\n",
        "        # After fully connected layer transformation, shape of outputs:\n",
        "        # (num_steps, batch_size, vocab_size)\n",
        "        outputs = self.dense(torch.cat(outputs, dim=0))\n",
        "        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,\n",
        "                                          enc_valid_lens]\n",
        "\n",
        "    @property\n",
        "    def attention_weights(self):\n",
        "        return self._attention_weights"
      ],
      "metadata": {
        "id": "RfssmDozP45e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = d2l.MTFraEng(batch_size=128)\n",
        "embed_size, num_hiddens, num_layers, dropout = 256, 256, 3, 0.2\n",
        "encoder = d2l.Seq2SeqEncoder(\n",
        "    len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "decoder = Seq2SeqAttentionDecoder(\n",
        "    len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
        "model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n",
        "                    lr=0.005)\n",
        "trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)\n",
        "trainer.fit(model, data)"
      ],
      "metadata": {
        "id": "ioNSlMfrSVGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}